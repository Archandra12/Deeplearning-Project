{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedding-Concept.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XJg255I93okzmYc5GmEJ21J2kQkZmWad",
      "authorship_tag": "ABX9TyNq3LlJcKVRzOAduoKRgvnV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archandra12/Deeplearning-Project/blob/codes/Embedding_Concept.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBWgjiLr3b95"
      },
      "source": [
        "##Practice tasks to understand word embeddings ad their use in RNN\n",
        "- Model can learn embeddings during Model training, or\n",
        "- Model can use a pretrained embeddings to train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PRSHaGV392O"
      },
      "source": [
        "We are taking movie ratings data which has 50000 reviews. They are 50% positive and 50% negative reviews. We are breaking data into two parts for training and Testing. Each set has equal proportion of positive or negative reivews.\n",
        "\n",
        "####Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01JXrqC_HwnN"
      },
      "source": [
        "#Pandas libraries\n",
        "import pandas as pds\n",
        "import numpy as np\n",
        "\n",
        "#keras libraries\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzWlvI8_TjDe",
        "outputId": "ced9addc-e927-498c-e326-776add476fe3"
      },
      "source": [
        "#method to load file\n",
        "def load_data(file):\n",
        "  sheet1 = pds.read_excel(file, sheet_name = 0)  \n",
        "  sheet2 = pds.read_excel(file, sheet_name = 1)  \n",
        "  data = pds.concat([sheet1, sheet2], axis=0)\n",
        "  return data\n",
        "#Loading Training Data\n",
        "newData = load_data('/content/drive/MyDrive/LSTM-Dataset/aclImdb/train/mergetest.xlsx')\n",
        "newData.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 25000 entries, 0 to 12499\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Label   25000 non-null  object\n",
            " 1   Review  25000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 585.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZuH19CHIQlq"
      },
      "source": [
        "####Separating reviews and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixsrFa2cUTHm"
      },
      "source": [
        "docs = newData.iloc[:, newData.columns!='Label']\n",
        "labels = newData.iloc[:, newData.columns!='Label']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "OjxRh1l8VyTk",
        "outputId": "8a94e4d2-1152-4f49-afd3-95daea69df55"
      },
      "source": [
        "docs.head(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Story of a man who has unnatural feelings for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This film lacked something I couldn't put my f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When I was little my parents took me along to ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review\n",
              "0  Story of a man who has unnatural feelings for ...\n",
              "1  Airport '77 starts as a brand new luxury 747 p...\n",
              "2  This film lacked something I couldn't put my f...\n",
              "3  Sorry everyone,,, I know this is supposed to b...\n",
              "4  When I was little my parents took me along to ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8njd76ws7Tei"
      },
      "source": [
        "####Cleaning and tokenizing the review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFwL2gbsWEfO",
        "outputId": "3f4d3ad0-ea80-44ed-ebc1-6066c1c191e2"
      },
      "source": [
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import string\n",
        "def tokenize_reviews(docs):\n",
        "  review_text = docs.iloc[:, docs.columns=='Review'].replace('--', ' ')\n",
        "  review_text = review_text['Review'].str.lower()\n",
        "  review_text = [c for c in review_text if c not in punctuation]\n",
        "  all_text2 = ' '.join(review_text)\n",
        "  #print(all_text2)\n",
        "  # create a list of words\n",
        "  words = all_text2.split()\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  words = [w.translate(table) for w in words]\n",
        "  words = [word for word in words if word.isalpha()]\n",
        "  # Count all the words using Counter Method\n",
        "  count_words = Counter(words)\n",
        "  total_words = len(words)\n",
        "  #we are taking top 10000 words based on their frequency of occurances\n",
        "  sorted_words = count_words.most_common(10000)\n",
        "  vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "  len(vocab_to_int)\n",
        "  encoded_docs = []\n",
        "  for review in docs['Review']:\n",
        "    r = []\n",
        "    for w in review.split():\n",
        "      if w in vocab_to_int:\n",
        "        r.append(vocab_to_int[w])\n",
        "      else:\n",
        "        r.append(0)      \n",
        "    encoded_docs.append(r)\n",
        "  return encoded_docs, vocab_to_int\n",
        "encoded_docs, vocab_to_int = tokenize_reviews(docs)\n",
        "print (len(encoded_docs[0]))\n",
        "print (encoded_docs[1])\n",
        "print (encoded_docs[2])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112\n",
            "[0, 0, 494, 14, 3, 3414, 154, 8383, 0, 1641, 6, 4765, 57, 16, 4355, 5719, 0, 135, 0, 5, 993, 4858, 0, 0, 0, 0, 36, 6, 1495, 97, 0, 3, 730, 4, 0, 5, 24, 3477, 7, 0, 4, 8, 106, 2998, 5, 1, 1038, 14, 3, 0, 78, 20, 2042, 6, 0, 573, 0, 0, 0, 0, 39, 0, 0, 8383, 0, 290, 126, 14, 4217, 18, 0, 1, 1641, 6, 0, 32, 1, 0, 0, 0, 0, 0, 24, 104, 0, 0, 0, 0, 0, 0, 0, 0, 36, 3666, 1, 5977, 0, 1020, 45, 16, 2661, 0, 34, 1272, 5, 1997, 1, 4355, 0, 0, 1496, 20, 3, 0, 1641, 3459, 20, 33, 4255, 1079, 18, 130, 244, 24, 4629, 0, 209, 1823, 33, 3199, 0, 7, 1, 0, 0, 1878, 1143, 4, 1, 1641, 5541, 8, 6480, 77, 1, 2023, 112, 8, 7837, 5, 1, 1319, 204, 4630, 7, 1, 743, 4, 1, 0, 0, 0, 986, 7, 345, 0, 1091, 0, 7, 0, 251, 0, 125, 0, 1927, 126, 258, 1, 686, 0, 15, 1, 0, 14, 34, 0, 325, 16, 61, 820, 604, 0, 0, 0, 611, 479, 1, 1037, 265, 0, 0, 0, 10, 323, 759, 5, 1, 0, 1704, 765, 0, 0, 13, 514, 32, 0, 0, 0, 130, 271, 172, 38, 0, 8092, 0, 0, 129, 0, 0, 6, 98, 422, 4, 1554, 347, 8, 6, 438, 254, 21, 2580, 15, 1, 204, 0, 0, 4, 1, 288, 0, 94, 0, 25, 107, 37, 233, 0, 155, 402, 10, 28, 1, 0, 0, 0, 43, 55, 1555, 113, 4, 1, 288, 16, 3, 313, 0, 0, 0, 91, 1, 6480, 0, 26, 67, 1, 3199, 0, 0, 6054, 4, 1, 0, 0, 1, 1322, 65, 257, 5, 2013, 1, 197, 0, 16, 153, 1034, 1704, 513, 4, 1, 825, 0, 0, 0, 0, 0, 0, 6, 112, 8, 2551, 348, 1, 128, 16, 3, 5102, 6481, 4256, 143, 2554, 0, 349, 0, 50, 1, 986, 1126, 45, 41, 0, 14, 1, 0, 0, 41, 44, 98, 4, 1, 3415, 23, 2998, 0, 0, 3, 522, 312, 11, 95, 25, 90, 15, 3, 84, 110, 1704, 513, 18, 83, 6772, 0, 749, 0, 0, 0, 0, 3, 145, 553, 4, 2335, 41, 857, 41, 1057, 778, 10, 6, 3, 1015, 0, 0, 1, 238, 0, 113, 892, 28, 2165, 15, 0, 1017, 232, 21, 11, 72, 551, 100, 1, 1641, 7837, 0, 0, 21, 14, 72, 8890, 14, 0, 198, 47, 138, 25, 0, 0, 50, 1, 0, 393, 555, 179, 0, 1256, 57, 11, 72, 16, 3, 162, 634, 4, 623, 3974, 0, 0, 1495, 42, 18, 0, 40, 139, 1834, 0, 0, 0, 14, 1, 0, 0, 4585, 0, 0, 6, 142, 18, 60, 201, 3, 362, 4, 136, 0, 1139, 53, 532, 227, 0, 5, 40, 163, 3718, 7, 1, 0, 0, 0, 342, 390, 0, 2161, 303, 4, 0, 0, 524, 0, 232, 130, 1, 0, 0, 1953, 735, 33, 1721, 554, 4, 898, 565, 3, 154, 609, 899, 0, 105, 51, 136, 16, 0, 0, 14, 0, 2043, 5, 2325, 45, 0, 1164, 2184, 136, 0, 1, 3886, 41, 153, 362, 4, 355, 2326, 565, 1, 0, 0, 0, 58, 38, 5, 67, 10, 1721, 898, 0, 236, 21, 242, 0, 95, 842, 140, 3, 771, 288, 554, 636, 4, 0, 0, 0, 846, 1, 19, 43, 1879, 904, 16, 503, 7838, 0, 7474, 1570, 0, 0, 76, 129, 56, 51, 80, 70, 1, 2965, 1641, 2162, 300, 0, 84, 0, 0, 16, 1, 80, 104, 0, 2185, 10, 290, 3015, 4, 268, 7, 1, 0, 0, 0, 4, 0, 254, 0, 68, 102, 4, 732, 4, 434, 94, 70, 10, 37, 0, 0, 0, 3, 110, 0, 0, 215, 136, 23, 3, 110, 749, 0, 1, 1040, 6, 588, 0, 21, 72, 2284, 41, 1057, 6, 5207, 59, 6, 3, 885, 14, 0, 0, 10, 95, 25, 74, 3, 178, 49, 19, 44, 90, 0, 0, 0, 358, 1184, 23, 2613, 44, 157, 0, 0, 111, 0, 0, 104, 61, 0, 2294, 0, 0, 43, 289, 230, 8, 13, 3, 1320, 5, 330, 7, 0, 28, 61, 0, 2294, 0, 0, 262, 164, 0, 0, 78, 28, 61, 0, 2294, 0, 0, 262, 1871, 130, 0, 0, 0, 6, 332, 110, 5, 81, 0, 47, 23, 915, 4, 80, 1052, 1534, 5, 163, 45, 15, 0, 0, 0, 0, 6, 1, 87, 1704, 0, 4, 1, 288, 0, 94, 37, 233, 0, 0, 402, 1, 969, 487, 8, 53, 44, 34, 65, 3, 220, 0, 1, 358, 0, 1845, 458, 0, 325, 148, 0, 3, 19, 42, 3, 0, 1641, 40, 0, 27, 10, 353, 41, 0, 0, 32, 0, 0, 0, 0, 0, 0]\n",
            "[0, 19, 3528, 139, 0, 0, 266, 55, 4462, 20, 30, 0, 3237, 20, 1, 170, 4, 1, 935, 0, 0, 4802, 5265, 5, 553, 4, 1131, 50, 54, 5208, 1, 291, 16, 39, 935, 0, 0, 1, 710, 136, 363, 601, 14, 106, 1432, 1, 151, 30, 0, 0, 95, 52, 73, 25, 74, 1, 171, 36, 0, 48, 26, 866, 35, 1, 0, 0, 40, 0, 0, 0, 0, 95, 8, 25, 74, 1, 0, 0, 586, 36, 13, 1, 9699, 7, 115, 0, 0, 442, 51, 0, 4, 24, 0, 1885, 2, 0, 2, 1055, 4, 301, 2, 24, 6055, 0, 70, 4, 1722, 41, 227, 0, 0, 109, 2255, 69, 26, 13, 7, 115, 16, 1, 0, 0, 0, 13, 695, 7, 10, 0, 0, 0, 826, 8, 13, 2314, 15, 33, 0, 37, 1830, 15, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Twi8yB6QNWE"
      },
      "source": [
        "###Encoding Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5xYiUrMZEPN",
        "outputId": "c63fba32-007e-489d-9834-320d7944fc3b"
      },
      "source": [
        "encoded_labels = [1 if label =='positive' else 0 for label in newData['Label']]\n",
        "encoded_labels = np.array(encoded_labels)\n",
        "encoded_labels"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrf3WT2hQZwP"
      },
      "source": [
        "###Preparing Input for Model\n",
        "Padding the encoded reviews to ensure all reviews are of same size in terms of number of tokens. We are keeping total number of tokens in each review as 500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imE1rx_oZbER",
        "outputId": "b60f1ad9-7f82-4a6a-d519-a5b8a8d15073"
      },
      "source": [
        "max_length = 500\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    4    3 ...    0    0    0]\n",
            " [   0   16  153 ...    0    0    0]\n",
            " [   0   19 3528 ...    0    0    0]\n",
            " ...\n",
            " [   0  236 2597 ...    0    0    0]\n",
            " [   0    0    0 ...    0    0    0]\n",
            " [   0  710  470 ...    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwg49KR_egdg",
        "outputId": "48a5fc6e-8069-4333-c14c-e9575f8170cf"
      },
      "source": [
        "padded_docs.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx7JnO4mSgWl"
      },
      "source": [
        "###Define Model - Embedding layer learns along the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHho801IaIYn",
        "outputId": "a4256a49-08df-45d7-f2b9-c95d19b0149c"
      },
      "source": [
        "# define the model - model is mainly trying to learn embedding nothing much\n",
        "vocab_size=10001 #needs to be 1 more than number of words in the vocab\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           320032    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 16001     \n",
            "=================================================================\n",
            "Total params: 336,033\n",
            "Trainable params: 336,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46BSIEibS8fs"
      },
      "source": [
        "###Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA-hFlWIa6J7",
        "outputId": "c3413ece-524f-4f18-95c2-549be510774e"
      },
      "source": [
        "# fit the model\n",
        "model.fit(padded_docs, encoded_labels, epochs=50, verbose=0, validation_split=0.2)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, encoded_labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 100.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnshWrfMTp_g"
      },
      "source": [
        "###Preparing a Model using pretrained Embeddings\n",
        " **Took glove.6B.100d.txt for this from GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fg3c4V8ujFq",
        "outputId": "8cf8d1b8-8763-437b-cb9f-609478dbed3b"
      },
      "source": [
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "embeddings_index = dict()\n",
        "f = open('/content/drive/MyDrive/LSTM-Dataset/aclImdb/train/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = zeros((10001, 100))\n",
        "for word, i in vocab_to_int.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_saFK-Z2xzg_",
        "outputId": "247c0452-4b4f-4120-e756-b8a93261daa6"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10001, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaAPggzV6Px"
      },
      "source": [
        "###Define Model with pre-trained Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtg3v_Lawo3T",
        "outputId": "ed2a7855-0b2f-4363-8548-4290a8001530"
      },
      "source": [
        "# define model\n",
        "vocab_size=10001\n",
        "model = Sequential()\n",
        "#vocab size = number of words in entire training dataset, 100 is number of weights for each word, input length = number of tokens in padded_reviews\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 100)          1000100   \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 50000)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 50001     \n",
            "=================================================================\n",
            "Total params: 1,050,101\n",
            "Trainable params: 50,001\n",
            "Non-trainable params: 1,000,100\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgA89G3xXtWm"
      },
      "source": [
        "###Training the model with pre-trained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJAuJPi90aYg",
        "outputId": "a4b1f7c1-cdec-4eba-f622-d3e6e8543e3a"
      },
      "source": [
        "# fit the model\n",
        "model.fit(padded_docs, encoded_labels, epochs=50, verbose=0, validation_split=0.2)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, encoded_labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 100.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9kvrCNPYeFd"
      },
      "source": [
        "###Checking Model performance on Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDXL3852YjtL",
        "outputId": "274e3134-136d-48f2-88ab-d08ad66d8217"
      },
      "source": [
        "#Loading Training Data\n",
        "testData = load_data('/content/drive/MyDrive/LSTM-Dataset/aclImdb/train/MovieRating_Test.xlsx')\n",
        "testData.info()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 24999 entries, 0 to 12499\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Label   24999 non-null  object\n",
            " 1   Review  24999 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 585.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BRTYqOeZu4p",
        "outputId": "3d429695-f1fe-451d-ec79-bdea498d4f23"
      },
      "source": [
        "encoded_test_docs, vocab_to_int = tokenize_reviews(testData)\n",
        "print (len(encoded_test_docs))\n",
        "print (encoded_test_docs[1])\n",
        "print (encoded_test_docs[2])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24999\n",
            "[0, 6, 33, 468, 4, 133, 1, 2224, 4, 224, 94, 23, 1, 0, 0, 3, 0, 0, 62, 163, 259, 145, 0, 0, 571, 455, 4, 1, 90, 0, 2058, 4, 0, 3, 0, 0, 0, 239, 0, 104, 214, 130, 11, 34, 23, 2062, 4, 0, 3, 113, 0, 0, 1094, 15, 10, 0, 141, 61, 0, 0, 0, 0, 39, 103, 0, 0, 0, 16, 0, 39, 0, 3107, 1, 0, 0, 0, 39, 0, 16, 0, 0, 3, 61, 1, 144, 0, 0, 2076, 7307, 405, 660, 155, 10, 19, 0, 30, 1, 0, 3, 0, 128, 1507, 48, 1, 1931, 0, 0, 13, 399, 7, 10, 0, 0, 133, 1, 1931, 122, 28, 208, 279, 1, 2396, 162, 0, 0, 0, 0, 164, 19, 0, 105, 15, 0, 0, 44, 88, 392, 1, 2396, 162, 1881, 0, 3, 30, 217, 7, 0, 24, 108, 0, 64, 96, 9, 668, 0, 0, 0, 10, 6, 0, 224, 0, 0, 23, 3788, 123, 94, 5, 0, 3, 41, 21, 62, 175, 5, 61, 10, 0, 103, 0, 0, 64, 6, 2621, 2, 0, 857, 18, 44, 123, 113, 3, 2, 123, 0, 0, 63, 143, 11, 96, 10, 30, 31, 259, 145, 13, 2, 523, 608, 20, 1, 369, 0, 1, 617, 13, 207, 0, 64, 271, 585, 5, 257, 56, 16, 1, 461, 19, 379, 0, 18, 22, 0, 0]\n",
            "[0, 4, 31, 0, 697, 138, 5470, 0, 36, 0, 485, 41, 34, 67, 2, 982, 9029, 459, 60, 0, 0, 34, 78, 6, 2280, 3, 1190, 239, 76, 3, 113, 38, 0, 319, 4, 0, 0, 0, 17, 0, 183, 51, 72, 774, 227, 5, 1188, 48, 6, 161, 20, 156, 0, 469, 30, 1, 7202, 0, 6, 22, 2, 691, 2094, 108, 7, 10, 0, 542, 16, 1, 6464, 0, 36, 6, 81, 1, 63, 27, 15, 330, 2, 0, 0, 0, 0, 3, 0, 0, 23, 185, 2678, 0, 3, 0, 108, 6, 40, 14, 71, 2, 1069, 14, 1, 0, 0, 0, 38, 88, 209, 35, 1, 0, 0, 0, 17, 6, 1113, 15, 1138, 588, 3, 0, 0, 0, 85, 1508, 142, 5694, 15, 2845, 1701, 0, 0, 6, 177, 71, 54, 115, 3, 9, 6, 2, 188, 453, 50, 21, 3728, 16, 0, 0, 0, 542, 35, 0, 3, 1, 6464, 205, 3, 1248, 75, 48, 34, 0, 0, 0, 63, 109, 306, 85, 11, 66, 485, 6, 1, 6464, 205, 3, 1, 7686, 18, 0, 149, 32, 0, 22, 649, 1500, 1073, 354, 0, 0, 0, 240, 35, 10, 610, 3, 103, 0, 624, 0, 3, 0, 0, 0, 0, 34, 25, 97, 21, 460, 0, 2, 284, 4, 434, 3, 163, 18, 144, 149, 7, 1, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXh8mtgqbR8Z",
        "outputId": "8d74623d-f775-4419-ec1c-93003bdc97bd"
      },
      "source": [
        "encoded_test_labels = [1 if label =='positive' else 0 for label in testData['Label']]\n",
        "encoded_test_labels = np.array(encoded_test_labels)\n",
        "encoded_test_labels"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn08q5MobSvr",
        "outputId": "5dd13d11-e78a-40c9-f20c-f358244335e9"
      },
      "source": [
        "max_length = 500\n",
        "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
        "print(padded_test_docs)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0 173   0 ...   0   0   0]\n",
            " [  0   6  33 ...   0   0   0]\n",
            " [  0   4  31 ...   0   0   0]\n",
            " ...\n",
            " [  0  47   0 ...  36 305   0]\n",
            " [  0   0  15 ...   0   0   0]\n",
            " [  0 105  10 ...   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxzCtNyfcNqn"
      },
      "source": [
        "###Predicting on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLX_R0Z1cRlW"
      },
      "source": [
        "#ynew = model.predict_classes(padded_test_docs)\n",
        "ynew = np.argmax(model.predict(padded_test_docs), axis=-1)\n",
        "# show the inputs and predicted outputs\n",
        "#for i in range(len(padded_test_docs)):\n",
        "#\tprint(\"X=%s, Labels=%s, Predicted=%s\" % (padded_test_docs[i], encoded_test_labels[i], ynew[i]))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VFp6rgngy5D",
        "outputId": "a4223834-ab3b-4e52-b7c4-6671f0eb3a5e"
      },
      "source": [
        "testData['Predicted_Label'] = ['negative' if i==0 else 'positive' for i in ynew]\n",
        "testData[testData['Label'] != testData['Predicted_Label']].shape, testData.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12500, 3), (24999, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du34cnZInmkm"
      },
      "source": [
        "##How to save and reuse models in Keras\n",
        "\n",
        "Model weights are saved to HDF5 format. This is a grid format that is ideal for storing multi-dimensional arrays of numbers\n",
        "\n",
        "The model structure can be described and saved using two different formats: JSON and YAML.\n",
        "\n",
        "In order to do that we need to install __h5py__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gduOGPeSnsJU",
        "outputId": "5e9ed143-4fb8-4548-eb9d-79065c983dd4"
      },
      "source": [
        "!sudo pip install h5py"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgRFcfL5qEvy"
      },
      "source": [
        "Saving Model in JASON and weights in HDF5 format (this helps in storing weights in multidimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSK92t-coMbm",
        "outputId": "c069317c-4885-4b22-bdf9-97b61caf1358"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"Embeddings_Conceptmodel.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"Embeddings_Conceptmodel.h5\")\n",
        "print(\"Saved model to disk\")\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMftB14mq0YP"
      },
      "source": [
        "If we want to create new model using saved JASON and weights We can do that as below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx7HcfakrCBp",
        "outputId": "746516dd-b243-4421-e336-172e6c997a47"
      },
      "source": [
        "# load json and create model\n",
        "json_file = open('/content/Embeddings_Conceptmodel.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/Embeddings_Conceptmodel.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "loaded_model.fit(padded_docs, encoded_labels, epochs=50, verbose=0, validation_split=0.2)\n",
        "score = loaded_model.evaluate(padded_docs, encoded_labels, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m53AH1mCtzlB"
      },
      "source": [
        "###Saving the Model in YAML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LSZB_RTt8yy",
        "outputId": "a4a57fe4-4fe2-4ec7-f831-3d30d186a0cd"
      },
      "source": [
        "!pip install PyYAML"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnkYox4ouFZw",
        "outputId": "bdd8d5af-973f-4545-c954-342d314ca29d"
      },
      "source": [
        "from keras.models import model_from_yaml\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "with open(\"Embeddings_Conceptmodel.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"Embeddings_Conceptmodel.h5\")\n",
        "print(\"Saved model to disk\")\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UrfwY83vJuW"
      },
      "source": [
        "###To load the model from yaml saved on local path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R28NvEtzvOfM",
        "outputId": "84befbab-04f9-422c-aaa4-79f31043121c"
      },
      "source": [
        "# load yaml and create model\n",
        "yaml_file = open('/content/Embeddings_Conceptmodel.yaml', 'r')\n",
        "loaded_model_yaml = yaml_file.read()\n",
        "yaml_file.close()\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"/content/Embeddings_Conceptmodel.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "loaded_model.fit(padded_docs, encoded_labels, epochs=50, verbose=0, validation_split=0.2)\n",
        "score = loaded_model.evaluate(padded_docs, encoded_labels, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQATqyOjxglM"
      },
      "source": [
        "###Save Model Weights and Architecture Together\n",
        "\n",
        "Keras also supports a simpler interface to save both the model weights and model architecture together into a single H5 file.\n",
        "\n",
        "Saving the model in this way includes everything we need to know about the model, including:\n",
        "\n",
        " - Model weights.\n",
        " - Model architecture.\n",
        " - Model compilation details (loss and metrics).\n",
        " - Model optimizer state.\n",
        " \n",
        "This means that we can load and use the model directly, without having to re-compile it as we did above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEaGnzbBxnXk"
      },
      "source": [
        "model.save(\"Embedding_Conceptmodel_Save.h5\")"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imq1WqfLzRNP"
      },
      "source": [
        "Need to perform following steps when we need to load model and use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k43svMTNzagM",
        "outputId": "301a2053-a2f1-42d8-b935-91383803f5d1"
      },
      "source": [
        "from keras.models import load_model \n",
        "# load model\n",
        "loaded_model = load_model('Embedding_Conceptmodel_Save.h5')\n",
        "# summarize model.\n",
        "loaded_model.summary()\n",
        "loaded_model.fit(padded_docs, encoded_labels, epochs=50, verbose=0, validation_split=0.2)\n",
        "score = loaded_model.evaluate(padded_docs, encoded_labels, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 100)          1000100   \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 50000)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 50001     \n",
            "=================================================================\n",
            "Total params: 1,050,101\n",
            "Trainable params: 50,001\n",
            "Non-trainable params: 1,000,100\n",
            "_________________________________________________________________\n",
            "accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X8qAgN1RsF9"
      },
      "source": [
        "##Text Analysis - Prediction on Republic\n",
        "\n",
        "A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence.\n",
        "\n",
        "Neural network models are a preferred method for developing statistical language models because they can use a distributed representation where different words with similar meanings have similar representation and because they can use a large context of recently observed words when making predictions.\n",
        "\n",
        "\n",
        "We will perform following:\n",
        "- Data Preparation\n",
        "- Train Language Model\n",
        "- Use Language Model\n",
        "\n",
        "Here is a direct link to the clean version of the data file:\n",
        "\n",
        "###Data Preparation\n",
        "We will start by preparing the data for modeling.\n",
        "\n",
        "We could see following from a quick look on the data:\n",
        "\n",
        " - Book/Chapter headings (e.g. “BOOK I.”).\n",
        " - British English spelling (e.g. “honoured”)\n",
        " - Lots of punctuation (e.g. “–“, “;–“, “?–“, and more)\n",
        " - Strange names (e.g. “Polemarchus”).\n",
        " - Some long monologues that go on for hundreds of lines.\n",
        " - Some quoted dialog (e.g. ‘…’)\n",
        " - These observations, and more, suggest at ways that we may wish to prepare the text data.\n",
        "\n",
        "\n",
        "###Language Model Design\n",
        "We will develop a model of the text that we can then use to generate new sequences of text.\n",
        "\n",
        "The language model will be statistical and will predict the probability of each word given an input sequence of text. The predicted word will be fed in as input to in turn generate the next word.\n",
        "\n",
        "__A key design decision is how long the input sequences should be. They need to be long enough to allow the model to learn the context for the words to predict. This input length will also define the length of seed text used to generate new sequences when we use the model.__\n",
        "\n",
        "There is no correct answer. With enough time and resources, we could explore the ability of the model to learn with differently sized input sequences. We will pick a length of 50 words for the length of the input sequences, somewhat arbitrarily.\n",
        "\n",
        "To keep the example brief, we will let all of the text flow together and train the model to predict the next word across sentences, paragraphs, and even books or chapters in the text.\n",
        "\n",
        "Now that we have a model design, we can look at transforming the raw text into sequences of 50 input words to 1 output word, ready to fit a model.\n",
        "\n",
        "Load Text\n",
        "The first step is to load the text into memory.\n",
        "\n",
        "We can develop a small function to load the entire text file into memory and return it. The function is called load_doc() and is listed below. Given a filename, it returns a sequence of loaded text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iucZgX_fRyMl"
      },
      "source": [
        "#/content/drive/MyDrive/LSTM-Dataset/republic_clean.txt\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQYgz63vSCg2",
        "outputId": "79e1c24a-417a-463d-c179-d900a8075086"
      },
      "source": [
        "# load document\n",
        "in_filename = '/content/drive/MyDrive/LSTM-Dataset/republic_clean.txt'\n",
        "doc = load_doc(in_filename)\n",
        "print(doc[:200])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOOK I.\n",
            "\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
            "Artemis.); and also because I wanted to see in what\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN0D1xR7TLut"
      },
      "source": [
        "###Clean Text\n",
        "We need to transform the raw text into a sequence of tokens or words that we can use as a source to train the model.\n",
        "\n",
        "Based on reviewing the raw text (above), below are some specific operations we will perform to clean the text. \n",
        "\n",
        " - Replace ‘–‘ with a white space so we can split words better.\n",
        " - Split words based on white space.\n",
        " - Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
        " - Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
        " - Normalize all words to lowercase to reduce the vocabulary size.\n",
        " - Vocabulary size is a big deal with language modeling. A smaller vocabulary results in a smaller model that trains faster.\n",
        "\n",
        "We can implement each of these cleaning operations in this order in a function. Below is the function clean_doc() that takes a loaded document as an argument and returns an array of clean tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_v3LtfjTPWV",
        "outputId": "91e5d45f-85a8-4d81-bfd5-8d1f631cf77e"
      },
      "source": [
        "import string\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\t# make lower case\n",
        "\ttokens = [word.lower() for word in tokens]\n",
        "\treturn tokens\n",
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
            "Total Tokens: 118685\n",
            "Unique Tokens: 7411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shZyayadW5pE"
      },
      "source": [
        "\n",
        "Next, we can look at shaping the tokens into sequences and saving them to file.\n",
        "\n",
        "Save Clean Text\n",
        "We can organize the long list of tokens into sequences of 50 input words and 1 output word.\n",
        "\n",
        "That is, sequences of 51 words.\n",
        "\n",
        "We can do this by iterating over the list of tokens from token 51 onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens.\n",
        "\n",
        "We will transform the tokens into space-separated strings for later storage in a file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD-I1FYpW8WW",
        "outputId": "acbf6009-ec85-4a3d-872e-6ff02ac16e09"
      },
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 118634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8xJ8OfDYmRg"
      },
      "source": [
        "####Saving the Data after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVWaitekYqoV"
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "# save sequences to file\n",
        "out_filename = '/content/drive/MyDrive/LSTM-Dataset/republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvwoasITy6K1"
      },
      "source": [
        "###Train Language Model\n",
        "We can now train a statistical language model from the prepared data.\n",
        "\n",
        "The model we will train is a neural language model. It has a few unique characteristics:\n",
        "\n",
        " - It uses a distributed representation for words so that different words with similar meanings will have a similar representation.\n",
        " - It learns the representation at the same time as learning the model.\n",
        " - It learns to predict the probability for the next word using the context of the last 100 words.\n",
        " \n",
        "Specifically, we will use an Embedding Layer to learn the representation of words, and a Long Short-Term Memory (LSTM) recurrent neural network to learn to predict words based on their context.\n",
        "\n",
        "Let’s start by loading our training data.\n",
        "\n",
        "Load Sequences\n",
        "We can load our training data using the load_doc() function we developed in the previous section.\n",
        "\n",
        "Once loaded, we can split the data into separate training sequences by splitting based on new lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOCDfUrEzHwC",
        "outputId": "066fb6f0-04a2-4f45-ebe3-0c2b0ff9fb8f"
      },
      "source": [
        "# load\n",
        "in_filename = '/content/drive/MyDrive/LSTM-Dataset/republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "lines[:3]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted',\n",
              " 'i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted with']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi6h_mmAgWM7"
      },
      "source": [
        "####Encode Sequences\n",
        "The word embedding layer expects input sequences to be comprised of integers.\n",
        "\n",
        "We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping.\n",
        "\n",
        "To do this encoding, we will use the __Tokenizer__ class in the Keras API.\n",
        "\n",
        "First, the Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "We can then use the fit Tokenizer to encode all of the training sequences, converting each sequence from a list of words to a list of integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doklgYJmbFEm"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5DT2Fn6jdmy",
        "outputId": "e032771f-83cf-455e-efd9-b602c55eda39"
      },
      "source": [
        "#to get the mapping of words to integer\n",
        "tokenizer.word_index\n",
        "#to know the length of vocabulary as we would need in embedding layer\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-R-XsXnnd0X"
      },
      "source": [
        "####Sequence Inputs and Output\n",
        "Now that we have encoded the input sequences, we need to separate them into input (X) and output (y) elements.\n",
        "\n",
        "We can do this with array slicing.\n",
        "\n",
        "After separating, we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.\n",
        "\n",
        "This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.\n",
        "\n",
        "Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.\n",
        "\n",
        "Finally, we need to specify to the Embedding layer how long input sequences are. We know that there are 50 words because we designed the model, but a good generic way to specify that is to use the second dimension (number of columns) of the input data’s shape. That way, if you change the length of sequences when preparing data, you do not need to change this data loading code; it is generic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-l9EwginiqO",
        "outputId": "18e2dc63-1e41-46bb-ea59-f91b7e6365dd"
      },
      "source": [
        "# separate into input and output\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(118634, 50) (118634, 7412)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHw4qg7tq-_M"
      },
      "source": [
        "###Fit Model\n",
        "We can now define and fit our language model on the training data.\n",
        "\n",
        "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
        "\n",
        "Common values are 50, 100, and 300. We will use 50 here, but consider testing smaller or larger values.\n",
        "\n",
        "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
        "\n",
        "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61J7nWmbrCGj",
        "outputId": "f11be50c-f3ca-49c8-c3b9-15ce7d9ea758"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 50, 50)            370600    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7412)              748612    \n",
            "=================================================================\n",
            "Total params: 1,270,112\n",
            "Trainable params: 1,270,112\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdjbQWH3siBI"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3cp7-Gosn-4",
        "outputId": "bfd2661c-46e5-41f9-d551-fb76e1e5e4cd"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=10)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "927/927 [==============================] - 187s 194ms/step - loss: 6.4866 - accuracy: 0.0631\n",
            "Epoch 2/10\n",
            "927/927 [==============================] - 180s 194ms/step - loss: 5.7156 - accuracy: 0.1039\n",
            "Epoch 3/10\n",
            "927/927 [==============================] - 180s 194ms/step - loss: 5.4571 - accuracy: 0.1262\n",
            "Epoch 4/10\n",
            "927/927 [==============================] - 180s 194ms/step - loss: 5.2962 - accuracy: 0.1428\n",
            "Epoch 5/10\n",
            "927/927 [==============================] - 180s 194ms/step - loss: 5.1751 - accuracy: 0.1526\n",
            "Epoch 6/10\n",
            "927/927 [==============================] - 180s 194ms/step - loss: 5.0760 - accuracy: 0.1601\n",
            "Epoch 7/10\n",
            "927/927 [==============================] - 180s 195ms/step - loss: 4.9840 - accuracy: 0.1656\n",
            "Epoch 8/10\n",
            "927/927 [==============================] - 181s 195ms/step - loss: 4.9305 - accuracy: 0.1691\n",
            "Epoch 9/10\n",
            "927/927 [==============================] - 181s 195ms/step - loss: 4.8480 - accuracy: 0.1715\n",
            "Epoch 10/10\n",
            "927/927 [==============================] - 180s 195ms/step - loss: 4.7862 - accuracy: 0.1752\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcf922ddc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPMkD-Z69kqB"
      },
      "source": [
        "###Save Model\n",
        "At the end of the run, the trained model is saved to file.\n",
        "\n",
        "Here, we use the Keras model API to save the model to the file ‘model.h5‘ in the current working directory.\n",
        "\n",
        "Later, when we load the model to make predictions, we will also need the mapping of words to integers. This is in the Tokenizer object, and we can save that too using Pickle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIf4jy3T8w0J"
      },
      "source": [
        "# save the model to file\n",
        "from pickle import dump\n",
        "model.save('/content/drive/MyDrive/LSTM-Dataset/languagemodel.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('/content/drive/MyDrive/LSTM-Dataset/tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE6mGLItAZIe"
      },
      "source": [
        "###Use Language Model\n",
        "Now that we have a trained language model, we can use it.\n",
        "\n",
        "In this case, we can use it to generate new sequences of text that have the same statistical properties as the source text.\n",
        "\n",
        "This is not practical, at least not for this example, but it gives a concrete example of what the language model has learned.\n",
        "\n",
        "We will start by loading the training sequences again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOI6JySPNwon"
      },
      "source": [
        "# load cleaned text sequences\n",
        "in_filename = '/content/drive/MyDrive/LSTM-Dataset/republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR0hvW4lOm-3"
      },
      "source": [
        "We need the text so that we can choose a source sequence as input to the model for generating a new sequence of text.\n",
        "\n",
        "The model will require 50 words as input.\n",
        "\n",
        "Later, we will need to specify the expected length of input. We can determine this from the input sequences by calculating the length of one line of the loaded data and subtracting 1 for the expected output word that is also on the same line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBD2QzbXOoDJ",
        "outputId": "d2c1c932-6b2c-438f-a6a0-04e0a154d0c7"
      },
      "source": [
        "seq_length = len(lines[0].split()) - 1\n",
        "seq_length"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntrqW5-AO0RL"
      },
      "source": [
        "###Load Model\n",
        "We can now load the model from file.\n",
        "\n",
        "Keras provides the load_model() function for loading the model, ready for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJBeVAS7O324"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "loaded_model = load_model('/content/drive/MyDrive/LSTM-Dataset/languagemodel.h5')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1SwxxLQP56j"
      },
      "source": [
        " Load the tokenizer from file using the Pickle API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuGF6w5QIQc"
      },
      "source": [
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/drive/MyDrive/LSTM-Dataset/tokenizer.pkl', 'rb'))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZoa2LFQRZ4t"
      },
      "source": [
        "###Generate Text\n",
        "The first step in generating text is preparing a seed input.\n",
        "\n",
        "We will select a random line of text from the input text for this purpose. Once selected, we will print it so that we have some idea of what was used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tis2RiIS1wW",
        "outputId": "4d1e495e-3164-4511-cfe9-66216ae57801"
      },
      "source": [
        "print(lines[0])\n",
        "print(lines[118633])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was\n",
            "one another and to the gods both while remaining here and when like conquerors in the games who go round to gather gifts we receive our reward and it s hall be well with us both in this life and in the pilgrimage of a thousand years which we have been\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rykjC5HRrFJ",
        "outputId": "03892194-6e4a-4a1e-f2c5-e54d9892a129"
      },
      "source": [
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(seed_text + '\\n')\n",
        "print(len(seed_text), len(lines))\n",
        "print(randint(0,len(lines)))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "many are their friends and for all these reasons they will be unwilling to waste their lands and rase their houses their enmity to them will only last until the many innocent sufferers have compelled the guilty few to give satisfaction i agree he said that our citizens should thus deal\n",
            "\n",
            "286 118634\n",
            "7975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR1zeJjVUjC3"
      },
      "source": [
        "Next, we can generate new words, one at a time.\n",
        "\n",
        "First, the seed text must be encoded to integers using the same tokenizer that we used when training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBXtTc2yUkBm"
      },
      "source": [
        "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YznpbOiga6oQ"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "encoded = pad_sequences([encoded], maxlen=seq_length)\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZOO2K6uWoaJ"
      },
      "source": [
        "The model can predict the next word directly by calling model.predict_classes() that will return the index of the word with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JByXHBu-WpeC"
      },
      "source": [
        "# predict probabilities for each word\n",
        "import numpy as np \n",
        "#yhat = loaded_model.predict_classes(encoded, verbose=0)\n",
        "yhat = np.argmax(model.predict(encoded), axis=-1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxl5OvkXlcGg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuyFV1bYW1JZ"
      },
      "source": [
        "We can then look up the index in the Tokenizers mapping to get the associated word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T0Q3L_PzW2q1",
        "outputId": "ebc0b90b-eb23-45d5-94bf-c0e6052ac0e7"
      },
      "source": [
        "out_word = ''\n",
        "for word, index in tokenizer.word_index.items():\n",
        "\tif index == yhat:\n",
        "\t\tout_word = word\n",
        "\t\tbreak\n",
        "out_word"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'to'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdT302PXztAE"
      },
      "source": [
        "Putting it all together as below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS8jN8Zpop75",
        "outputId": "b5cf6f7e-e914-456b-f464-28b7caca923f"
      },
      "source": [
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "  #print(in_text)\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "    \n",
        "    yhat = np.argmax(model.predict(encoded), axis=-1)    \n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        #print(yhat, out_word)\n",
        "        break\n",
        "    in_text += ' ' + out_word\n",
        "    #print(in_text)\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)  \n",
        "# generate new text\n",
        "print('Original text:')\n",
        "print(seed_text)\n",
        "generated = generate_seq(loaded_model, tokenizer, seq_length, seed_text, 50)\n",
        "print('Generated text:')\n",
        "print(generated)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original text:\n",
            "many are their friends and for all these reasons they will be unwilling to waste their lands and rase their houses their enmity to them will only last until the many innocent sufferers have compelled the guilty few to give satisfaction i agree he said that our citizens should thus deal\n",
            "Generated text:\n",
            "to the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the same and the\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}